{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "619",
      "language": "python",
      "name": "619"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "metadata": {
      "interpreter": {
        "hash": "cbfaa0c31130d6fe797b1b315fd7af716006f03c19477746c82c11dfd72f132a"
      }
    },
    "colab": {
      "name": "trainer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUd2GkDO6M7a"
      },
      "source": [
        "Install Libs"
      ],
      "id": "qUd2GkDO6M7a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTG5Q6H-6M75",
        "outputId": "c888bcb0-aec9-4329-f726-36bcc59a8023"
      },
      "source": [
        "!pip install wandb --upgrade\n",
        "!pip install pytorch-lightning\n",
        "!pip install albumentations\n",
        "!pip install python-dotenv\n",
        "!pip install torchmetrics"
      ],
      "id": "cTG5Q6H-6M75",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d5/5d/20ab24504de2669c9a76a50c9bdaeb44a440b0e5e4b92be881ed323857b1/wandb-0.10.26-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied, skipping upgrade: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 20.6MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 17.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (54.2.0)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 5.9MB/s \n",
            "\u001b[?25hCollecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: pathtools, subprocess32\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=8974320ab61f356a3d477563808c4ff4fd491928a87edbdad75873470e8de22e\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=7049de35f20eba6564a728c38cba9ca558d3f680b99de9ca78e95d47d1743b78\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "Successfully built pathtools subprocess32\n",
            "Installing collected packages: shortuuid, docker-pycreds, sentry-sdk, pathtools, smmap, gitdb, GitPython, subprocess32, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.26\n",
            "Collecting pytorch-lightning\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/99/68da5c6ca999de560036d98c492e507d17996f5eeb7e76ba64acd4bbb142/pytorch_lightning-1.2.8-py3-none-any.whl (841kB)\n",
            "\u001b[K     |████████████████████████████████| 849kB 4.0MB/s \n",
            "\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 38.9MB/s \n",
            "\u001b[?25hCollecting fsspec[http]>=0.8.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 40.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n",
            "Collecting torchmetrics>=0.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n",
            "\u001b[K     |████████████████████████████████| 184kB 41.4MB/s \n",
            "\u001b[?25hCollecting future>=0.17.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n",
            "\u001b[K     |████████████████████████████████| 829kB 31.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.4.1)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n",
            "Collecting aiohttp; extra == \"http\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3MB 29.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.7/dist-packages (from fsspec[http]>=0.8.1->pytorch-lightning) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (54.2.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.28.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n",
            "Requirement already satisfied: chardet<5.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (3.0.4)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 52.9MB/s \n",
            "\u001b[?25hCollecting async-timeout<4.0,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n",
            "Collecting yarl<2.0,>=1.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n",
            "\u001b[K     |████████████████████████████████| 296kB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (1.24.3)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.10.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n",
            "Building wheels for collected packages: PyYAML, future\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=86e43bf51bfad876778575a7b76acd0328de5aab7c8999b1cd6e879184cfee54\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=822b136e44933c524dc0b84534d95aa429735f88171991e5cecfbb238a1bc8a0\n",
            "  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n",
            "Successfully built PyYAML future\n",
            "Installing collected packages: PyYAML, multidict, async-timeout, yarl, aiohttp, fsspec, torchmetrics, future, pytorch-lightning\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: future 0.16.0\n",
            "    Uninstalling future-0.16.0:\n",
            "      Successfully uninstalled future-0.16.0\n",
            "Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.8 torchmetrics-0.2.0 yarl-1.6.3\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from albumentations) (4.1.2.30)\n",
            "Collecting imgaug<0.2.7,>=0.2.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
            "\u001b[K     |████████████████████████████████| 634kB 5.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (0.16.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations) (1.15.0)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.1.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (3.2.2)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (1.3.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (2.4.7)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations) (4.4.2)\n",
            "Building wheels for collected packages: imgaug\n",
            "  Building wheel for imgaug (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for imgaug: filename=imgaug-0.2.6-cp37-none-any.whl size=654019 sha256=9625a7e30f5255896890f123f65631cda0cc569b75c9b29b1a3ceb9d570c4e34\n",
            "  Stored in directory: /root/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
            "Successfully built imgaug\n",
            "Installing collected packages: imgaug\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "Successfully installed imgaug-0.2.6\n",
            "Collecting python-dotenv\n",
            "  Downloading https://files.pythonhosted.org/packages/86/62/aacbd1489fc6026c9278e9fd5cfdd49c5b1b5375d3c97a624e8f8f999eb2/python_dotenv-0.17.0-py2.py3-none-any.whl\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-0.17.0\n",
            "Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.2.0)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRyEHAVr6M79"
      },
      "source": [
        "Download Dataset"
      ],
      "id": "hRyEHAVr6M79"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3BpivW86M7_",
        "outputId": "50151c07-25f5-473b-f570-224ffd5df7f3"
      },
      "source": [
        "# https://drive.google.com/file/d//view?usp=sharing\n",
        "!wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\" -O \"download.tar.gz\" && rm -rf /tmp/cookies.txt\n",
        "!tar -xvf download.tar.gz"
      ],
      "id": "P3BpivW86M7_",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-04-18 07:39:23--  https://docs.google.com/uc?export=download&confirm=oWSY&id=1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5\n",
            "Resolving docs.google.com (docs.google.com)... 108.177.13.113, 108.177.13.139, 108.177.13.101, ...\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.13.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download [following]\n",
            "--2021-04-18 07:39:23--  https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download\n",
            "Resolving doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)... 173.194.210.132, 2607:f8b0:400c:c0f::84\n",
            "Connecting to doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://docs.google.com/nonceSigner?nonce=9mea3qvncuors&continue=https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=1co8i5l2u9ott7tdcoo7gqmkg5g34n0h [following]\n",
            "--2021-04-18 07:39:23--  https://docs.google.com/nonceSigner?nonce=9mea3qvncuors&continue=https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e%3Ddownload&hash=1co8i5l2u9ott7tdcoo7gqmkg5g34n0h\n",
            "Connecting to docs.google.com (docs.google.com)|108.177.13.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=9mea3qvncuors&user=14717700746067836179Z&hash=sgrtm03kr9vlirvru0an77vbqf0066uj [following]\n",
            "--2021-04-18 07:39:23--  https://doc-08-b4-docs.googleusercontent.com/docs/securesc/f87173j84nknam54nf1abrma76pnttik/h95sg3sd6lfg02il84r3oli48uafe29b/1618731525000/18183255587859120126/14717700746067836179Z/1mQCNp8dq499qnJI0YCc0hTVfmm4ppxO5?e=download&nonce=9mea3qvncuors&user=14717700746067836179Z&hash=sgrtm03kr9vlirvru0an77vbqf0066uj\n",
            "Connecting to doc-08-b4-docs.googleusercontent.com (doc-08-b4-docs.googleusercontent.com)|173.194.210.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-gzip]\n",
            "Saving to: ‘download.tar.gz’\n",
            "\n",
            "download.tar.gz         [                <=> ]   3.53G  33.9MB/s    in 46s     \n",
            "\n",
            "2021-04-18 07:40:09 (78.7 MB/s) - ‘download.tar.gz’ saved [3793541120]\n",
            "\n",
            "./Dataset_-17.3375dB/\n",
            "./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_highFreqChVal.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_trainInpLoc.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_labelVal.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_trainOutLoc.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_maxRateVal.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_valInpLoc.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_inpVal.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_valOutLoc.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_highFreqChTrain.npy\n",
            "./Dataset_-17.3375dB/-17.3375dB_maxRateTrain.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Uvjb6Qy6M8A"
      },
      "source": [
        "# Utils\n"
      ],
      "id": "_Uvjb6Qy6M8A"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ySUctsbP6M8B"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from typing import Optional, Tuple\n",
        "\n",
        "import pdb\n",
        "\n",
        "# def calc_acc(pred: torch.tensor, y: torch.tensor, num_classes: int, return_class_wise_acc: bool = False):    pred = pred.argmax(1)\n",
        "#     class_wise_acc = []\n",
        "#     for i in range(num_classes):\n",
        "#         tp = ((pred == i) & (y == i)).sum().float()\n",
        "#         tn = ((pred != i) & (y != i)).sum().float()\n",
        "#         fp = ((pred == i) & (y != i)).sum().float()\n",
        "#         fn = ((pred != i) & (y == i)).sum().float()\n",
        "#         acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "#         class_wise_acc.append(acc)\n",
        "    \n",
        "# #     pdb.set_trace()\n",
        "#     class_wise_acc = torch.Tensor(class_wise_acc)\n",
        "#     if return_class_wise_acc:\n",
        "#         return class_wise_acc\n",
        "#     return class_wise_acc.mean()"
      ],
      "id": "ySUctsbP6M8B",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GC3Y3gQ56M8C"
      },
      "source": [
        "# Dataset"
      ],
      "id": "GC3Y3gQ56M8C"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bT9wU26w6M8D"
      },
      "source": [
        "import glob\n",
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "from PIL import Image\n",
        "from typing import Optional, Tuple"
      ],
      "id": "bT9wU26w6M8D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxlK0ula6M8E"
      },
      "source": [
        "class BeamPredictionDataset(Dataset):\n",
        "    \"\"\"Beam Prediction dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, file_path: str,\n",
        "                 label_file_path: str,\n",
        "                 reshape: bool=False,\n",
        "                 transforms: Optional[transforms.Compose] = None,\n",
        "                 preprocessing_fn: Optional[transforms.Compose] = None) -> None:\n",
        "        \"\"\"\n",
        "        Init the Dataset\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.label_file_path = label_file_path\n",
        "        \n",
        "        self.data = np.load(file_path)\n",
        "        self.data = self.data.transpose((1, 0))\n",
        "        \n",
        "        self.label = np.load(label_file_path)\n",
        "#         pdb.set_trace()\n",
        "        \n",
        "        assert len(self.label) == len(self.data)\n",
        "        # reshape is true\n",
        "        # num_users x num_channels x num_antennas x real/imaginary\n",
        "        if reshape:\n",
        "            self.data = self.data.reshape((2, 4, 32, -1))\n",
        "            self.data = self.data.transpose((3, 2, 1, 0))\n",
        "            \n",
        "        self.preprocessing_fn = preprocessing_fn\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Returns the total length of dataset.\n",
        "        \"\"\"\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Gets an item from dataset\n",
        "        \"\"\"\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        user_data = self.data[idx]\n",
        "        \n",
        "        if self.preprocessing_fn is not None:\n",
        "            user_data = self.preprocessing_fn(user_data)\n",
        "        \n",
        "        if self.transforms is not None:\n",
        "            user_data = self.transforms(user_data)\n",
        "        \n",
        "        label = self.label[idx]\n",
        "        label = torch.Tensor(label).type(torch.int64) - 1\n",
        "        return user_data, label\n",
        "    \n",
        "def transform(x: np.array) -> torch.Tensor:\n",
        "    # mean normalize\n",
        "    x -= x.mean()\n",
        "    x /= x.std()\n",
        "    x = torch.Tensor(x)\n",
        "    return x"
      ],
      "id": "hxlK0ula6M8E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtxKPX_96M8F",
        "outputId": "4081e1da-d2fa-4822-a8da-ea867f0a5681"
      },
      "source": [
        "# example ds\n",
        "\n",
        "ds = BeamPredictionDataset(\n",
        "    file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
        "    label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
        "    transforms=transform\n",
        ")\n",
        "it = iter(ds)\n",
        "out = next(it)\n",
        "print(out)"
      ],
      "id": "MtxKPX_96M8F",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(tensor([ 1.1344,  1.1642,  0.8118,  0.1093, -0.2929,  0.3619,  1.0790,  1.3874,\n",
            "         1.2894,  1.1469,  1.3742,  0.6998, -0.4009,  0.3625,  0.8699,  1.1849,\n",
            "         0.6786,  1.1753,  1.1628,  1.0550, -0.8943, -0.7125,  0.3022,  1.0900,\n",
            "         0.8706,  1.3478,  1.5702,  0.9558, -1.6566, -0.7382,  0.0148,  0.6442,\n",
            "         0.3017,  0.8905,  0.9791,  1.1644, -0.9876, -0.7922, -0.4629,  0.3374,\n",
            "        -0.0516,  0.6355,  1.2137,  1.3104, -1.2456, -1.1153, -1.0029,  0.0712,\n",
            "        -0.3255,  0.3647,  1.0288,  1.1176, -1.1510, -1.2242, -1.2269, -0.4542,\n",
            "        -0.7970, -0.4256,  0.9413,  1.4918, -0.9748, -1.5398, -1.6161, -0.4450,\n",
            "        -1.1006, -0.9013,  0.4272,  1.0205, -0.8424, -1.2304, -1.5746, -1.4980,\n",
            "        -1.5050, -0.9478, -0.2943,  0.4626, -0.4176, -1.3170, -1.5857, -1.3130,\n",
            "        -1.2471, -1.1989, -0.6621,  0.2399, -0.2827, -1.0135, -1.6343, -1.5665,\n",
            "        -1.4390, -1.5518, -0.7836, -0.4450, -0.0606, -0.6873, -1.3370, -2.0944,\n",
            "        -1.3866, -1.2450, -1.3194, -0.6425,  0.7051, -0.0322, -1.0300, -1.2306,\n",
            "        -0.6418, -1.6214, -1.3991, -0.9170,  0.9509,  0.4661, -0.6956, -0.8484,\n",
            "        -0.9170, -1.3425, -1.5632, -1.4795,  1.4220,  0.5489,  0.4026, -0.5844,\n",
            "        -0.6274, -1.2085, -1.7199, -1.2470,  1.5720,  1.2481,  0.3709,  0.0025,\n",
            "         0.3596, -0.8391, -1.3560, -1.4393,  1.5227,  1.0183,  0.8746, -0.1959,\n",
            "         0.3806, -0.0644, -1.1502, -1.6148,  1.1727,  1.3091,  1.0675,  0.4314,\n",
            "         1.0033,  0.0746, -0.6809, -1.2830,  1.1760,  1.6166,  1.5842,  0.6260,\n",
            "         1.1363,  0.2335, -0.0576, -1.0852,  0.6769,  1.3474,  1.4323,  1.1049,\n",
            "         1.5561,  0.5584,  0.0851, -0.4431,  0.5574,  0.8314,  1.7251,  0.9947,\n",
            "         1.3336,  1.2545,  0.2566, -0.4365, -0.1540,  0.6379,  1.1497,  1.0744,\n",
            "         1.1799,  1.3945,  0.7851,  0.2137, -0.2622,  0.1544,  1.4368,  1.8656,\n",
            "         1.3089,  1.3896,  1.0659,  0.2861, -0.3666,  0.1218,  0.7796,  1.2413,\n",
            "         1.2102,  1.3221,  1.0344,  0.9304, -1.0877, -0.1499,  0.3859,  1.0914,\n",
            "         0.4617,  0.7131,  1.6144,  1.1880, -1.0281, -1.3422,  0.1930,  0.9728,\n",
            "         0.1454,  0.8132,  1.3767,  0.8216, -1.3210, -0.6237, -0.2125, -0.0454,\n",
            "        -0.3330,  0.3443,  0.7524,  0.9035, -1.2856, -0.2620, -0.4970, -0.2679,\n",
            "        -0.8429,  0.5761,  0.9023,  1.2007, -1.4464, -1.1691, -0.6407, -0.2095,\n",
            "        -0.7193,  0.1202,  0.4905,  1.4410, -1.2580, -1.2274, -1.1168, -0.4818,\n",
            "        -0.5149, -0.4690,  0.3170,  0.8014, -0.6843, -1.1411, -0.9833, -0.9430,\n",
            "        -0.6822, -0.7225, -0.5029,  0.7130, -0.4280, -0.8024, -1.1280, -0.9917]), tensor([36]))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V47uyDo06M8G"
      },
      "source": [
        "# Model"
      ],
      "id": "V47uyDo06M8G"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_VRLMlf6M8H"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "import pytorch_lightning as pl\n",
        "import wandb\n",
        "from argparse import ArgumentParser\n",
        "from typing import Tuple\n",
        "from torchmetrics import Accuracy, Precision, Recall\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib import pyplot as plt\n",
        "import pdb\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class BeamClassifier(pl.LightningModule):\n",
        "\n",
        "    def __init__(self, hparams) -> None:\n",
        "        \"\"\"\n",
        "        Downloading Backbone and defining structure of model.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # args from argparser\n",
        "        self.hparams = hparams\n",
        "        in_ch = self.hparams.in_ch\n",
        "        out_ch = self.hparams.out_ch\n",
        "        \n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(in_ch, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(2048, 2048),\n",
        "            nn.BatchNorm1d(2048),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            \n",
        "            nn.Linear(2048, out_ch),\n",
        "        )\n",
        "        self._acc_metric = Accuracy()\n",
        "        self._top3_acc_metric = Accuracy(top_k=3)\n",
        "        self._precision = Precision(average='macro', \n",
        "                                   num_classes=self.hparams.out_ch)\n",
        "        self._recall = Recall(average='macro', \n",
        "                             num_classes=self.hparams.out_ch)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\" Forward step of model.\n",
        "        \"\"\"\n",
        "#         pdb.set_trace()\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def loss_fn(self, pred: torch.Tensor, y: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Loss function used for model\"\"\"\n",
        "        y_sq = y.squeeze(-1)\n",
        "        loss = F.cross_entropy(pred, y_sq)\n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self) -> torch.optim:\n",
        "        # REQUIRED\n",
        "        # can return multiple optimizers and learning_rate schedulers\n",
        "        opt = torch.optim.Adam(self.model.parameters(),\n",
        "                               lr=self.hparams.learning_rate)\n",
        "        lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "            opt, self.hparams.max_nb_epochs, self.hparams.learning_rate)\n",
        "        self.lr_scheduler = lr_scheduler\n",
        "#         lr_scheduler = None\n",
        "        return [opt], [lr_scheduler]\n",
        "\n",
        "    def train_dataloader(self) -> DataLoader:\n",
        "        \"\"\"Define the data loader for training data\"\"\"\n",
        "        # REQUIRED\n",
        "        return DataLoader(BeamPredictionDataset(\n",
        "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpTrain.npy',\n",
        "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelTrain.npy',\n",
        "                                transforms=transform\n",
        "                            ),\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          num_workers=self.hparams.num_workers,\n",
        "                          shuffle=True)\n",
        "\n",
        "    def training_step(self, batch: list, batch_idx: int) -> dict:\n",
        "        \"\"\"Backward step of model\"\"\"\n",
        "        # REQUIRED\n",
        "        x, y = batch\n",
        "        pred = self.forward(x)\n",
        "\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        \n",
        "        pred = F.softmax(pred, dim=-1)\n",
        "        y_sq = y.squeeze(-1)\n",
        "        \n",
        "        # metrics\n",
        "        acc = self._acc_metric(pred, y_sq)\n",
        "        prec = self._precision(pred, y_sq)\n",
        "        rec = self._recall(pred, y_sq)\n",
        "        \n",
        "        if self.lr_scheduler is not None:\n",
        "            lr = self.lr_scheduler.get_last_lr()[0]\n",
        "\n",
        "        if(batch_idx % self.hparams.wandb_log_num_iter == 0):\n",
        "            wandb.log({\n",
        "                'train_loss': loss,\n",
        "            })\n",
        "            \n",
        "        return {\n",
        "            'loss': loss,\n",
        "            'train_acc': acc,\n",
        "            'train_prec': prec,\n",
        "            'train_rec': rec\n",
        "        }\n",
        "    \n",
        "    def training_epoch_end(self, outputs: list) -> None:\n",
        "        acc = torch.stack([x['train_acc'] for x in outputs]).mean()\n",
        "        prec = torch.stack([x['train_prec'] for x in outputs]).mean()\n",
        "        rec = torch.stack([x['train_rec'] for x in outputs]).mean()\n",
        "        \n",
        "        if self.lr_scheduler is not None:\n",
        "            lr = self.lr_scheduler.get_last_lr()[0]\n",
        "        else:\n",
        "            lr = self.hparams.learning_rate\n",
        "        logs = {\n",
        "            'lr': lr,\n",
        "            'train_acc': acc,\n",
        "            'train_prec': prec,\n",
        "            'train_rec': rec\n",
        "        }\n",
        "        wandb.log(logs)\n",
        "        self.log_dict(logs)\n",
        "    \n",
        "    def val_dataloader(self) -> DataLoader:\n",
        "        \"\"\"Define the data loader for validation data\"\"\"\n",
        "        # OPTIONAL\n",
        "        return DataLoader(BeamPredictionDataset(\n",
        "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
        "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
        "                                transforms=transform\n",
        "                            ),\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          num_workers=self.hparams.num_workers\n",
        "                         )\n",
        "\n",
        "    def validation_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
        "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
        "        # REQUIRED\n",
        "        # pdb.set_trace()\n",
        "        x, y = batch\n",
        "        \n",
        "        pred = self.forward(x)\n",
        "\n",
        "        loss = self.loss_fn(pred, y)\n",
        "        \n",
        "        pred = F.softmax(pred, dim=-1)\n",
        "        y_sq = y.squeeze(-1)\n",
        "        \n",
        "        acc = self._acc_metric(pred, y_sq)\n",
        "        prec = self._precision(pred, y_sq)\n",
        "        rec = self._recall(pred, y_sq)\n",
        "\n",
        "        return {\n",
        "            'val_loss': loss,\n",
        "            'val_acc': acc,\n",
        "            'val_prec': prec,\n",
        "            'val_rec': rec\n",
        "        }\n",
        "\n",
        "    def validation_epoch_end(self, outputs: list) -> None:\n",
        "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
        "        val_loss = torch.stack([x['val_loss'] for x in outputs]).mean()\n",
        "        val_acc = torch.stack([x['val_acc'] for x in outputs]).mean()\n",
        "        val_prec = torch.stack([x['val_prec'] for x in outputs]).mean()\n",
        "        val_rec = torch.stack([x['val_rec'] for x in outputs]).mean()\n",
        "        \n",
        "        logs = {\n",
        "            'val_loss': val_loss,\n",
        "            'val_acc': val_acc,\n",
        "            'val_prec': val_prec,\n",
        "            'val_rec': val_rec\n",
        "        }\n",
        "        wandb.log(logs)\n",
        "        self.log_dict(logs)\n",
        "    \n",
        "    def test_dataloader(self) -> DataLoader:\n",
        "        \"\"\"Define the data loader for test data\"\"\"\n",
        "        print(\"Test Dataloader\")\n",
        "        # OPTIONAL\n",
        "        return DataLoader(BeamPredictionDataset(\n",
        "                                file_path='./Dataset_-17.3375dB/-17.3375dB_inpVal.npy',\n",
        "                                label_file_path='./Dataset_-17.3375dB/-17.3375dB_labelVal.npy',\n",
        "                                transforms=transform\n",
        "                            ),\n",
        "                          batch_size=self.hparams.batch_size,\n",
        "                          num_workers=self.hparams.num_workers\n",
        "                         )\n",
        "    \n",
        "    def test_step(self, batch: list, batch_idx: torch.Tensor) -> dict:\n",
        "        \"\"\"Validation step to be carried out on validation data.\"\"\"\n",
        "        # REQUIRED\n",
        "        # pdb.set_trace()\n",
        "        x, y = batch\n",
        "        \n",
        "        pred = self.forward(x)        \n",
        "        pred = F.softmax(pred, dim=-1)\n",
        "        y_sq = y.squeeze(-1)\n",
        "        logs = {\n",
        "            'pred': pred,\n",
        "            'ground_truth': y_sq\n",
        "        }\n",
        "#         self.log_dict(logs)\n",
        "        return logs\n",
        "    \n",
        "    def test_epoch_end(self, outputs: list) -> None:\n",
        "        \"\"\"Use results from each validation step to generate validation stats at epoch end\"\"\"\n",
        "        pred = torch.cat([x['pred'] for x in outputs], dim=0)\n",
        "        ground_truth = torch.cat([x['ground_truth'] for x in outputs], dim=0)\n",
        "\n",
        "        top1_acc = self._acc_metric(pred, ground_truth)\n",
        "        top3_acc = self._top3\n",
        "        _acc_metric(pred, ground_truth)\n",
        "        \n",
        "#         pdb.set_trace()\n",
        "        conf_matrix = confusion_matrix(ground_truth.tolist(), \n",
        "                                       pred.argmax(-1).tolist(), \n",
        "                                       labels=list(range(self.hparams.out_ch)))\n",
        "        df_cm = pd.DataFrame(conf_matrix, index = [str(i) for i in range(self.hparams.out_ch)],\n",
        "                  columns = [str(i) for i in range(self.hparams.out_ch)])\n",
        "        \n",
        "        plt.figure(figsize = (50,50))\n",
        "        ax = sns.heatmap(df_cm, annot=True)\n",
        "        logs = {\n",
        "            'top1_acc': top1_acc,\n",
        "            'top3_acc': top3_acc,\n",
        "            'conf_matrix': wandb.Image(ax)\n",
        "        }\n",
        "        wandb.log(logs)\n",
        "    \n",
        "\n",
        "    def load_encoder_weights(self) -> None:\n",
        "        \"\"\"Loads encoder weights from ckpt\"\"\"\n",
        "        ckpt = torch.load(self.hparams.encoder_ckpt_path)\n",
        "        pretrained_dict = ckpt['state_dict']\n",
        "        model_dict = self.state_dict()\n",
        "        pretrained_dict = {k: v for k, v in pretrained_dict.items() if (\n",
        "            'encoder' in k) and (k in model_dict)}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        self.load_state_dict(model_dict)\n",
        "\n",
        "    def load_model_weights_from_ckpt(self) -> None:\n",
        "        \"\"\"Load model weights to model on cpu\"\"\"\n",
        "        ckpt = torch.load(self.hparams.model_ckpt_path,\n",
        "                          map_location=torch.device('cpu'))\n",
        "        pretrained_dict = ckpt['state_dict']\n",
        "        model_dict = self.state_dict()\n",
        "        pretrained_dict = {k: v for k,\n",
        "                           v in pretrained_dict.items() if (k in model_dict)}\n",
        "        model_dict.update(pretrained_dict)\n",
        "        self.load_state_dict(model_dict)\n",
        "        \n",
        "    def _get_learning_rate(self) -> float:\n",
        "        i = 0\n",
        "        for param_group in self._optimizer.param_groups:\n",
        "            if i == 0:\n",
        "                learning_rate = param_group[\"lr\"]\n",
        "            else:\n",
        "                if learning_rate != param_group[\"lr\"]:\n",
        "                    raise ValueError(\n",
        "                        \"different param groups have different lr\")\n",
        "        return learning_rate\n",
        "\n",
        "    @staticmethod\n",
        "    def add_model_specific_args(parent_parser: ArgumentParser) -> ArgumentParser:\n",
        "        \"\"\"\n",
        "        Specify the hyperparams for this LightningModule\n",
        "        \"\"\"\n",
        "        # MODEL specific arguments\n",
        "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
        "        parser.add_argument('--learning_rate', default=0.02, type=float)\n",
        "        parser.add_argument('--batch_size', default=32, type=int)\n",
        "        parser.add_argument('--in_ch', default=256, type=int)\n",
        "        parser.add_argument('--out_ch', default=64, type=int)\n",
        "        parser.add_argument('--num_workers', default=1, type=int)\n",
        "        parser.add_argument('--max_nb_epochs', default=1, type=int)\n",
        "        return parser"
      ],
      "id": "I_VRLMlf6M8H",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-87vNjb6M8W",
        "outputId": "a95fefa2-9322-4448-8fbd-5def45022f3e"
      },
      "source": [
        "from argparse import ArgumentParser, Namespace\n",
        "args_str = [\n",
        "        # model related args\n",
        "        '--max_nb_epochs=1',\n",
        "        '--learning_rate=1e-3',\n",
        "        '--batch_size=16',\n",
        "        '--in_ch=256',\n",
        "        '--out_ch=64',\n",
        "        '--num_workers=2'\n",
        "]\n",
        "parser = ArgumentParser(add_help=False)\n",
        "parser = BeamClassifier.add_model_specific_args(parser)\n",
        "args= parser.parse_args(args_str)\n",
        "\n",
        "model = BeamClassifier(args)\n",
        "train_dl = model.train_dataloader()\n",
        "it = iter(train_dl)\n",
        "x, y = next(it)\n",
        "\n",
        "\n",
        "pred = model(x)\n",
        "loss = model.loss_fn(pred, y)\n",
        "print(loss)"
      ],
      "id": "_-87vNjb6M8W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(4.3884, grad_fn=<NllLossBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiWDaCDp6M8b"
      },
      "source": [
        "# Trainer"
      ],
      "id": "fiWDaCDp6M8b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SqrrWNIW6M8c"
      },
      "source": [
        "args_str = ['--tpu_cores=0',\n",
        "        '--progress_bar_refresh_rate=20',\n",
        "        '--wandb_run_name=baseline',\n",
        "        '--wandb_project_name=Beam Prediction',\n",
        "        '--wandb_log_num_iter=1',\n",
        "        '--gpus=1',\n",
        "        # model related args\n",
        "        '--max_nb_epochs=100',\n",
        "        '--learning_rate=1e-2',\n",
        "        '--batch_size=256',\n",
        "        '--in_ch=256',\n",
        "        '--out_ch=64',\n",
        "    ]"
      ],
      "id": "SqrrWNIW6M8c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTMtcytv6M8c",
        "outputId": "538a1cd0-9e58-4b31-9652-a5782bc2ee1d"
      },
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "PROJECT_ROOT = os.path.dirname(os.path.abspath('.'))\n",
        "load_dotenv(dotenv_path=os.path.join(PROJECT_ROOT, '.env'))"
      ],
      "id": "vTMtcytv6M8c",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zcv7wxiD6M8d",
        "outputId": "df86b54c-cfa0-444b-f08b-97d867ecf31f"
      },
      "source": [
        "from pytorch_lightning import Trainer, seed_everything\n",
        "from argparse import ArgumentParser, Namespace\n",
        "import wandb\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "\n",
        "# import pdb\n",
        "\n",
        "parser = ArgumentParser(add_help=False)\n",
        "parser.add_argument('-wandb_run_name',\n",
        "                '--wandb_run_name',\n",
        "                help='Name of Wandb Run',\n",
        "                default='run',\n",
        "                type=str)\n",
        "parser.add_argument('-wandb_project_name',\n",
        "                    '--wandb_project_name',\n",
        "                    help='Wandb Project Name',\n",
        "                    default='deep_dream',\n",
        "                    type=str)\n",
        "parser.add_argument('-model_ckpt_path',\n",
        "                    '--model_ckpt_path',\n",
        "                    help='Model Checkpoint Path',\n",
        "                    default='./ckpts/model.ckpt',\n",
        "                    type=str)\n",
        "parser.add_argument('-wandb_log_num_iter',\n",
        "                    '--wandb_log_num_iter',\n",
        "                    help='After how many batches, we will log in training loop',\n",
        "                    default=1,\n",
        "                    type=int)\n",
        "parser.add_argument('-init_ckpt',\n",
        "                    '--init_ckpt',\n",
        "                    help='Initial Ckpt',\n",
        "                    default=None,\n",
        "                    type=str)\n",
        "\n",
        "def main(args):\n",
        "    \"\"\"Main function that will perform all the training\"\"\"\n",
        "    # init module\n",
        "    model = BeamClassifier(args)\n",
        "\n",
        "    # Using Wandblogger so that we can log our results to wandb\n",
        "    wandb.init(name=args.wandb_run_name,\n",
        "               project=args.wandb_project_name,\n",
        "               config=vars(args))\n",
        "        \n",
        "    wandb.watch(model)\n",
        "\n",
        "    # most basic trainer, uses good defaults\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_loss',\n",
        "        dirpath='./ckpts',\n",
        "        filename='{epoch:02d}-{val_loss:.2f}'\n",
        "    )\n",
        "    trainer = Trainer(logger=[], \n",
        "                      gpus=args.gpus, \n",
        "                      max_epochs=args.max_nb_epochs, \n",
        "                      resume_from_checkpoint=args.init_ckpt)\n",
        "#     pdb.set_trace()\n",
        "    trainer.fit(model)\n",
        "    trainer.test(model)\n",
        "    \n",
        "    ckpt_path = os.path.join('./ckpt', f\"{args.wandb_project_name}\", f\"{args.wandb_run_name}.ckpt\")\n",
        "    ckpt_base_path = os.path.dirname(ckpt_path)\n",
        "    trainer.save_checkpoint(ckpt_path)\n",
        "    wandb.save(ckpt_path)\n",
        "\n",
        "    return model\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # auto add args from trainer\n",
        "    parser = Trainer.add_argparse_args(parser)\n",
        "\n",
        "    # give the module a chance to add own params\n",
        "    # good practice to define LightningModule speficic params in the module\n",
        "    parser = BeamClassifier.add_model_specific_args(parser)\n",
        "\n",
        "    # parse params\n",
        "    args= parser.parse_args(args_str)\n",
        "\n",
        "    seed_everything(123)\n",
        "\n",
        "    model = main(args)"
      ],
      "id": "Zcv7wxiD6M8d",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Global seed set to 123\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:1bv02b6w) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 6344<br/>Program ended successfully."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Problem finishing run\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1502, in _atexit_cleanup\n",
            "    self._on_finish()\n",
            "  File \"/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/wandb/sdk/wandb_run.py\", line 1659, in _on_finish\n",
            "    self._backend.interface.publish_telemetry(self._telemetry_obj)\n",
            "  File \"/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 231, in publish_telemetry\n",
            "    self._publish(rec)\n",
            "  File \"/home/ubuntu/miniconda3/envs/619/lib/python3.8/site-packages/wandb/sdk/interface/interface.py\", line 524, in _publish\n",
            "    raise Exception(\"The wandb backend process has shutdown\")\n",
            "Exception: The wandb backend process has shutdown\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "...Successfully finished last run (ID:1bv02b6w). Initializing new run:<br/><br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                Tracking run with wandb version 0.10.26<br/>\n",
              "                Syncing run <strong style=\"color:#cdcd00\">baseline</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
              "                Project page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction</a><br/>\n",
              "                Run page: <a href=\"https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/4cjp0hlk\" target=\"_blank\">https://wandb.ai/sagarkaushik98/Beam%20Prediction/runs/4cjp0hlk</a><br/>\n",
              "                Run data is saved locally in <code>/home/ubuntu/619/wandb/run-20210417_183706-4cjp0hlk</code><br/><br/>\n",
              "            "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name             | Type       | Params\n",
            "------------------------------------------------\n",
            "0 | model            | Sequential | 17.5 M\n",
            "1 | _acc_metric      | Accuracy   | 0     \n",
            "2 | _top2_acc_metric | Accuracy   | 0     \n",
            "3 | _precision       | Precision  | 0     \n",
            "4 | _recall          | Recall     | 0     \n",
            "------------------------------------------------\n",
            "17.5 M    Trainable params\n",
            "0         Non-trainable params\n",
            "17.5 M    Total params\n",
            "69.853    Total estimated model params size (MB)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0:  70%|██████▉   | 298/426 [00:08<00:03, 36.25it/s, loss=1.82, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 0:  71%|███████   | 301/426 [00:08<00:03, 36.00it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  72%|███████▏  | 306/426 [00:08<00:03, 36.12it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  73%|███████▎  | 311/426 [00:08<00:03, 36.26it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  74%|███████▍  | 316/426 [00:08<00:03, 36.40it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  75%|███████▌  | 321/426 [00:08<00:02, 36.55it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  77%|███████▋  | 326/426 [00:08<00:02, 36.69it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  78%|███████▊  | 331/426 [00:08<00:02, 36.82it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  79%|███████▉  | 336/426 [00:09<00:02, 36.94it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  80%|████████  | 341/426 [00:09<00:02, 37.01it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  81%|████████  | 346/426 [00:09<00:02, 37.15it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  82%|████████▏ | 351/426 [00:09<00:02, 37.28it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  84%|████████▎ | 356/426 [00:09<00:01, 37.40it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  85%|████████▍ | 361/426 [00:09<00:01, 37.53it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  86%|████████▌ | 366/426 [00:09<00:01, 37.57it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  87%|████████▋ | 371/426 [00:09<00:01, 37.57it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  88%|████████▊ | 376/426 [00:10<00:01, 37.55it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  89%|████████▉ | 381/426 [00:10<00:01, 37.57it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  91%|█████████ | 386/426 [00:10<00:01, 37.68it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  92%|█████████▏| 391/426 [00:10<00:00, 37.78it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  93%|█████████▎| 396/426 [00:10<00:00, 37.89it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  94%|█████████▍| 401/426 [00:10<00:00, 37.91it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  95%|█████████▌| 406/426 [00:10<00:00, 38.01it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  96%|█████████▋| 411/426 [00:10<00:00, 38.10it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  98%|█████████▊| 416/426 [00:10<00:00, 38.20it/s, loss=1.82, v_num=]\n",
            "Epoch 0:  99%|█████████▉| 421/426 [00:10<00:00, 38.30it/s, loss=1.82, v_num=]\n",
            "Epoch 0: 100%|██████████| 426/426 [00:11<00:00, 38.16it/s, loss=1.82, v_num=]\n",
            "Epoch 1:  70%|██████▉   | 298/426 [00:07<00:03, 38.01it/s, loss=1.44, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 1:  70%|███████   | 300/426 [00:07<00:03, 37.73it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  72%|███████▏  | 305/426 [00:08<00:03, 37.80it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  73%|███████▎  | 310/426 [00:08<00:03, 37.93it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  74%|███████▍  | 315/426 [00:08<00:02, 38.04it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  75%|███████▌  | 320/426 [00:08<00:02, 38.17it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  76%|███████▋  | 325/426 [00:08<00:02, 38.17it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  77%|███████▋  | 330/426 [00:08<00:02, 38.17it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  79%|███████▊  | 335/426 [00:08<00:02, 38.13it/s, loss=1.44, v_num=]\n",
            "Validating:  29%|██▉       | 37/128 [00:00<00:02, 38.65it/s]\u001b[A\n",
            "Epoch 1:  80%|███████▉  | 340/426 [00:08<00:02, 38.09it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  81%|████████  | 345/426 [00:09<00:02, 38.06it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  82%|████████▏ | 350/426 [00:09<00:01, 38.03it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  83%|████████▎ | 355/426 [00:09<00:01, 38.00it/s, loss=1.44, v_num=]\n",
            "Validating:  45%|████▍     | 57/128 [00:01<00:01, 36.38it/s]\u001b[A\n",
            "Epoch 1:  85%|████████▍ | 360/426 [00:09<00:01, 37.95it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  86%|████████▌ | 365/426 [00:09<00:01, 37.92it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  87%|████████▋ | 370/426 [00:09<00:01, 37.88it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  88%|████████▊ | 375/426 [00:09<00:01, 37.84it/s, loss=1.44, v_num=]\n",
            "Validating:  60%|██████    | 77/128 [00:02<00:01, 35.46it/s]\u001b[A\n",
            "Epoch 1:  89%|████████▉ | 380/426 [00:10<00:01, 37.80it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  90%|█████████ | 385/426 [00:10<00:01, 37.77it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  92%|█████████▏| 390/426 [00:10<00:00, 37.73it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  93%|█████████▎| 395/426 [00:10<00:00, 37.69it/s, loss=1.44, v_num=]\n",
            "Validating:  76%|███████▌  | 97/128 [00:02<00:00, 34.99it/s]\u001b[A\n",
            "Epoch 1:  94%|█████████▍| 400/426 [00:10<00:00, 37.67it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  95%|█████████▌| 405/426 [00:10<00:00, 37.67it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  96%|█████████▌| 410/426 [00:10<00:00, 37.70it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  97%|█████████▋| 415/426 [00:10<00:00, 37.80it/s, loss=1.44, v_num=]\n",
            "Epoch 1:  99%|█████████▊| 420/426 [00:11<00:00, 37.90it/s, loss=1.44, v_num=]\n",
            "Epoch 1: 100%|██████████| 426/426 [00:11<00:00, 37.80it/s, loss=1.44, v_num=]\n",
            "Epoch 2:  70%|██████▉   | 298/426 [00:08<00:03, 35.68it/s, loss=1.38, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 2:  70%|███████   | 300/426 [00:08<00:03, 35.36it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  72%|███████▏  | 305/426 [00:08<00:03, 35.51it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  73%|███████▎  | 310/426 [00:08<00:03, 35.55it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  74%|███████▍  | 315/426 [00:08<00:03, 35.59it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  75%|███████▌  | 320/426 [00:09<00:02, 35.55it/s, loss=1.38, v_num=]\n",
            "Validating:  17%|█▋        | 22/128 [00:00<00:03, 35.21it/s]\u001b[A\n",
            "Epoch 2:  76%|███████▋  | 325/426 [00:09<00:02, 35.55it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  77%|███████▋  | 330/426 [00:09<00:02, 35.69it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  79%|███████▊  | 335/426 [00:09<00:02, 35.82it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  80%|███████▉  | 340/426 [00:09<00:02, 35.95it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  81%|████████  | 345/426 [00:09<00:02, 36.08it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  82%|████████▏ | 350/426 [00:09<00:02, 36.21it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  83%|████████▎ | 355/426 [00:09<00:01, 36.35it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  85%|████████▍ | 360/426 [00:09<00:01, 36.37it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  86%|████████▌ | 365/426 [00:10<00:01, 36.48it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  87%|████████▋ | 370/426 [00:10<00:01, 36.61it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  88%|████████▊ | 375/426 [00:10<00:01, 36.72it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  89%|████████▉ | 380/426 [00:10<00:01, 36.83it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  90%|█████████ | 385/426 [00:10<00:01, 36.94it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  92%|█████████▏| 390/426 [00:10<00:00, 37.05it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  93%|█████████▎| 395/426 [00:10<00:00, 37.12it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  94%|█████████▍| 400/426 [00:10<00:00, 37.23it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  95%|█████████▌| 405/426 [00:10<00:00, 37.33it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  96%|█████████▌| 410/426 [00:10<00:00, 37.41it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  97%|█████████▋| 415/426 [00:11<00:00, 37.36it/s, loss=1.38, v_num=]\n",
            "Epoch 2:  99%|█████████▊| 420/426 [00:11<00:00, 37.38it/s, loss=1.38, v_num=]\n",
            "Epoch 2: 100%|██████████| 426/426 [00:11<00:00, 37.25it/s, loss=1.38, v_num=]\n",
            "Epoch 3:  70%|██████▉   | 298/426 [00:08<00:03, 35.80it/s, loss=1.28, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 3:  70%|███████   | 300/426 [00:08<00:03, 35.56it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  72%|███████▏  | 305/426 [00:08<00:03, 35.70it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  73%|███████▎  | 310/426 [00:08<00:03, 35.84it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  74%|███████▍  | 315/426 [00:08<00:03, 35.98it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  75%|███████▌  | 320/426 [00:08<00:02, 36.13it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  76%|███████▋  | 325/426 [00:08<00:02, 36.27it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  77%|███████▋  | 330/426 [00:09<00:02, 36.41it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  79%|███████▊  | 335/426 [00:09<00:02, 36.54it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  80%|███████▉  | 340/426 [00:09<00:02, 36.67it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  81%|████████  | 345/426 [00:09<00:02, 36.80it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  82%|████████▏ | 350/426 [00:09<00:02, 36.92it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  83%|████████▎ | 355/426 [00:09<00:01, 36.96it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  85%|████████▍ | 360/426 [00:09<00:01, 37.08it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  86%|████████▌ | 365/426 [00:09<00:01, 37.19it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  87%|████████▋ | 370/426 [00:09<00:01, 37.30it/s, loss=1.28, v_num=]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 3:  88%|████████▊ | 375/426 [00:10<00:01, 37.40it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  89%|████████▉ | 380/426 [00:10<00:01, 37.51it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  90%|█████████ | 385/426 [00:10<00:01, 37.62it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  92%|█████████▏| 390/426 [00:10<00:00, 37.61it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  93%|█████████▎| 395/426 [00:10<00:00, 37.71it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  94%|█████████▍| 400/426 [00:10<00:00, 37.81it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  95%|█████████▌| 405/426 [00:10<00:00, 37.83it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  96%|█████████▌| 410/426 [00:10<00:00, 37.90it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  97%|█████████▋| 415/426 [00:10<00:00, 38.00it/s, loss=1.28, v_num=]\n",
            "Epoch 3:  99%|█████████▊| 420/426 [00:11<00:00, 38.10it/s, loss=1.28, v_num=]\n",
            "Epoch 3: 100%|█████████▉| 425/426 [00:11<00:00, 38.20it/s, loss=1.28, v_num=]\n",
            "Epoch 3: 100%|██████████| 426/426 [00:11<00:00, 37.99it/s, loss=1.28, v_num=]\n",
            "Epoch 4:  70%|██████▉   | 298/426 [00:08<00:03, 34.94it/s, loss=1.27, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 4:  70%|███████   | 300/426 [00:08<00:03, 34.63it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  72%|███████▏  | 305/426 [00:08<00:03, 34.68it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  73%|███████▎  | 310/426 [00:08<00:03, 34.84it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  74%|███████▍  | 315/426 [00:09<00:03, 34.99it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  75%|███████▌  | 320/426 [00:09<00:03, 35.15it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  76%|███████▋  | 325/426 [00:09<00:02, 35.28it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  77%|███████▋  | 330/426 [00:09<00:02, 35.35it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  79%|███████▊  | 335/426 [00:09<00:02, 35.36it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  80%|███████▉  | 340/426 [00:09<00:02, 35.36it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  81%|████████  | 345/426 [00:09<00:02, 35.49it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  82%|████████▏ | 350/426 [00:09<00:02, 35.62it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  83%|████████▎ | 355/426 [00:09<00:01, 35.72it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  85%|████████▍ | 360/426 [00:10<00:01, 35.86it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  86%|████████▌ | 365/426 [00:10<00:01, 35.96it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  87%|████████▋ | 370/426 [00:10<00:01, 36.05it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  88%|████████▊ | 375/426 [00:10<00:01, 36.15it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  89%|████████▉ | 380/426 [00:10<00:01, 36.19it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  90%|█████████ | 385/426 [00:10<00:01, 36.24it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  92%|█████████▏| 390/426 [00:10<00:00, 36.23it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  93%|█████████▎| 395/426 [00:10<00:00, 36.21it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  94%|█████████▍| 400/426 [00:11<00:00, 36.22it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  95%|█████████▌| 405/426 [00:11<00:00, 36.34it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  96%|█████████▌| 410/426 [00:11<00:00, 36.43it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  97%|█████████▋| 415/426 [00:11<00:00, 36.53it/s, loss=1.27, v_num=]\n",
            "Epoch 4:  99%|█████████▊| 420/426 [00:11<00:00, 36.63it/s, loss=1.27, v_num=]\n",
            "Epoch 4: 100%|██████████| 426/426 [00:11<00:00, 36.55it/s, loss=1.27, v_num=]\n",
            "Epoch 5:  70%|██████▉   | 298/426 [00:09<00:03, 32.88it/s, loss=1.22, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 5:  70%|███████   | 300/426 [00:09<00:03, 32.69it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  72%|███████▏  | 305/426 [00:09<00:03, 32.86it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  73%|███████▎  | 310/426 [00:09<00:03, 33.03it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  74%|███████▍  | 315/426 [00:09<00:03, 33.19it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  75%|███████▌  | 320/426 [00:09<00:03, 33.33it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  76%|███████▋  | 325/426 [00:09<00:03, 33.44it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  77%|███████▋  | 330/426 [00:09<00:02, 33.60it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  79%|███████▊  | 335/426 [00:09<00:02, 33.76it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  80%|███████▉  | 340/426 [00:10<00:02, 33.87it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  81%|████████  | 345/426 [00:10<00:02, 34.02it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  82%|████████▏ | 350/426 [00:10<00:02, 34.17it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  83%|████████▎ | 355/426 [00:10<00:02, 34.31it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  85%|████████▍ | 360/426 [00:10<00:01, 34.46it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  86%|████████▌ | 365/426 [00:10<00:01, 34.59it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  87%|████████▋ | 370/426 [00:10<00:01, 34.73it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  88%|████████▊ | 375/426 [00:10<00:01, 34.85it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  89%|████████▉ | 380/426 [00:10<00:01, 34.98it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  90%|█████████ | 385/426 [00:10<00:01, 35.07it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  92%|█████████▏| 390/426 [00:11<00:01, 35.19it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  93%|█████████▎| 395/426 [00:11<00:00, 35.31it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  94%|█████████▍| 400/426 [00:11<00:00, 35.43it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  95%|█████████▌| 405/426 [00:11<00:00, 35.52it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  96%|█████████▌| 410/426 [00:11<00:00, 35.64it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  97%|█████████▋| 415/426 [00:11<00:00, 35.74it/s, loss=1.22, v_num=]\n",
            "Epoch 5:  99%|█████████▊| 420/426 [00:11<00:00, 35.85it/s, loss=1.22, v_num=]\n",
            "Epoch 5: 100%|█████████▉| 425/426 [00:11<00:00, 35.97it/s, loss=1.22, v_num=]\n",
            "Epoch 5: 100%|██████████| 426/426 [00:11<00:00, 35.79it/s, loss=1.22, v_num=]\n",
            "Epoch 6:  70%|██████▉   | 298/426 [00:08<00:03, 35.98it/s, loss=1.21, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 6:  70%|███████   | 300/426 [00:08<00:03, 35.74it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  72%|███████▏  | 305/426 [00:08<00:03, 35.89it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  73%|███████▎  | 310/426 [00:08<00:03, 36.04it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  74%|███████▍  | 315/426 [00:08<00:03, 36.07it/s, loss=1.21, v_num=]\n",
            "Validating:  13%|█▎        | 17/128 [00:00<00:02, 38.93it/s]\u001b[A\n",
            "Epoch 6:  75%|███████▌  | 320/426 [00:08<00:02, 36.06it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  76%|███████▋  | 325/426 [00:09<00:02, 36.06it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  77%|███████▋  | 330/426 [00:09<00:02, 36.15it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  79%|███████▊  | 335/426 [00:09<00:02, 36.14it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  80%|███████▉  | 340/426 [00:09<00:02, 36.16it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  81%|████████  | 345/426 [00:09<00:02, 36.28it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  82%|████████▏ | 350/426 [00:09<00:02, 36.38it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  83%|████████▎ | 355/426 [00:09<00:01, 36.51it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  85%|████████▍ | 360/426 [00:09<00:01, 36.64it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  86%|████████▌ | 365/426 [00:09<00:01, 36.77it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  87%|████████▋ | 370/426 [00:10<00:01, 36.89it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  88%|████████▊ | 375/426 [00:10<00:01, 36.97it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  89%|████████▉ | 380/426 [00:10<00:01, 37.06it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  90%|█████████ | 385/426 [00:10<00:01, 37.17it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  92%|█████████▏| 390/426 [00:10<00:00, 37.26it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  93%|█████████▎| 395/426 [00:10<00:00, 37.35it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  94%|█████████▍| 400/426 [00:10<00:00, 37.46it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  95%|█████████▌| 405/426 [00:10<00:00, 37.56it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  96%|█████████▌| 410/426 [00:10<00:00, 37.66it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  97%|█████████▋| 415/426 [00:10<00:00, 37.75it/s, loss=1.21, v_num=]\n",
            "Epoch 6:  99%|█████████▊| 420/426 [00:11<00:00, 37.85it/s, loss=1.21, v_num=]\n",
            "Epoch 6: 100%|█████████▉| 425/426 [00:11<00:00, 37.90it/s, loss=1.21, v_num=]\n",
            "Epoch 6: 100%|██████████| 426/426 [00:11<00:00, 37.69it/s, loss=1.21, v_num=]\n",
            "Epoch 7:  70%|██████▉   | 298/426 [00:07<00:03, 37.81it/s, loss=1.22, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 7:  70%|███████   | 300/426 [00:07<00:03, 37.53it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  72%|███████▏  | 305/426 [00:08<00:03, 37.63it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  73%|███████▎  | 310/426 [00:08<00:03, 37.69it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  74%|███████▍  | 315/426 [00:08<00:02, 37.81it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  75%|███████▌  | 320/426 [00:08<00:02, 37.95it/s, loss=1.22, v_num=]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 7:  76%|███████▋  | 325/426 [00:08<00:02, 38.08it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  77%|███████▋  | 330/426 [00:08<00:02, 38.20it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  79%|███████▊  | 335/426 [00:08<00:02, 38.29it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  80%|███████▉  | 340/426 [00:08<00:02, 38.41it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  81%|████████  | 345/426 [00:08<00:02, 38.53it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  82%|████████▏ | 350/426 [00:09<00:01, 38.64it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  83%|████████▎ | 355/426 [00:09<00:01, 38.75it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  85%|████████▍ | 360/426 [00:09<00:01, 38.86it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  86%|████████▌ | 365/426 [00:09<00:01, 38.97it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  87%|████████▋ | 370/426 [00:09<00:01, 39.08it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  88%|████████▊ | 375/426 [00:09<00:01, 39.17it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  89%|████████▉ | 380/426 [00:09<00:01, 39.27it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  90%|█████████ | 385/426 [00:09<00:01, 39.36it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  92%|█████████▏| 390/426 [00:09<00:00, 39.45it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  93%|█████████▎| 395/426 [00:09<00:00, 39.54it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  94%|█████████▍| 400/426 [00:10<00:00, 39.51it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  95%|█████████▌| 405/426 [00:10<00:00, 39.42it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  96%|█████████▌| 410/426 [00:10<00:00, 39.47it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  97%|█████████▋| 415/426 [00:10<00:00, 39.56it/s, loss=1.22, v_num=]\n",
            "Epoch 7:  99%|█████████▊| 420/426 [00:10<00:00, 39.64it/s, loss=1.22, v_num=]\n",
            "Epoch 7: 100%|█████████▉| 425/426 [00:10<00:00, 39.69it/s, loss=1.22, v_num=]\n",
            "Epoch 7: 100%|██████████| 426/426 [00:10<00:00, 39.46it/s, loss=1.22, v_num=]\n",
            "Epoch 8:  70%|██████▉   | 298/426 [00:08<00:03, 36.02it/s, loss=1.14, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 8:  70%|███████   | 300/426 [00:08<00:03, 35.76it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  72%|███████▏  | 305/426 [00:08<00:03, 35.91it/s, loss=1.14, v_num=]\n",
            "Validating:   5%|▌         | 7/128 [00:00<00:03, 34.93it/s]\u001b[A\n",
            "Epoch 8:  73%|███████▎  | 310/426 [00:08<00:03, 35.95it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  74%|███████▍  | 315/426 [00:08<00:03, 35.97it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  75%|███████▌  | 320/426 [00:08<00:02, 36.07it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  76%|███████▋  | 325/426 [00:08<00:02, 36.21it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  77%|███████▋  | 330/426 [00:09<00:02, 36.35it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  79%|███████▊  | 335/426 [00:09<00:02, 36.36it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  80%|███████▉  | 340/426 [00:09<00:02, 36.36it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  81%|████████  | 345/426 [00:09<00:02, 36.34it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  82%|████████▏ | 350/426 [00:09<00:02, 36.32it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  83%|████████▎ | 355/426 [00:09<00:01, 36.39it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  85%|████████▍ | 360/426 [00:09<00:01, 36.51it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  86%|████████▌ | 365/426 [00:09<00:01, 36.63it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  87%|████████▋ | 370/426 [00:10<00:01, 36.70it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  88%|████████▊ | 375/426 [00:10<00:01, 36.80it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  89%|████████▉ | 380/426 [00:10<00:01, 36.91it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  90%|█████████ | 385/426 [00:10<00:01, 36.99it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  92%|█████████▏| 390/426 [00:10<00:00, 37.11it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  93%|█████████▎| 395/426 [00:10<00:00, 37.21it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  94%|█████████▍| 400/426 [00:10<00:00, 37.31it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  95%|█████████▌| 405/426 [00:10<00:00, 37.31it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  96%|█████████▌| 410/426 [00:10<00:00, 37.29it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  97%|█████████▋| 415/426 [00:11<00:00, 37.27it/s, loss=1.14, v_num=]\n",
            "Epoch 8:  99%|█████████▊| 420/426 [00:11<00:00, 37.36it/s, loss=1.14, v_num=]\n",
            "Epoch 8: 100%|█████████▉| 425/426 [00:11<00:00, 37.39it/s, loss=1.14, v_num=]\n",
            "Epoch 8: 100%|██████████| 426/426 [00:11<00:00, 37.18it/s, loss=1.14, v_num=]\n",
            "Epoch 9:  70%|██████▉   | 298/426 [00:08<00:03, 34.85it/s, loss=1.15, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 9:  70%|███████   | 300/426 [00:08<00:03, 34.62it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  72%|███████▏  | 305/426 [00:08<00:03, 34.78it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  73%|███████▎  | 310/426 [00:08<00:03, 34.91it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  74%|███████▍  | 315/426 [00:08<00:03, 35.06it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  75%|███████▌  | 320/426 [00:09<00:03, 35.21it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  76%|███████▋  | 325/426 [00:09<00:02, 35.35it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  77%|███████▋  | 330/426 [00:09<00:02, 35.50it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  79%|███████▊  | 335/426 [00:09<00:02, 35.50it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  80%|███████▉  | 340/426 [00:09<00:02, 35.51it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  81%|████████  | 345/426 [00:09<00:02, 35.64it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  82%|████████▏ | 350/426 [00:09<00:02, 35.78it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  83%|████████▎ | 355/426 [00:09<00:01, 35.91it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  85%|████████▍ | 360/426 [00:09<00:01, 36.04it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  86%|████████▌ | 365/426 [00:10<00:01, 36.16it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  87%|████████▋ | 370/426 [00:10<00:01, 36.23it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  88%|████████▊ | 375/426 [00:10<00:01, 36.34it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  89%|████████▉ | 380/426 [00:10<00:01, 36.45it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  90%|█████████ | 385/426 [00:10<00:01, 36.57it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  92%|█████████▏| 390/426 [00:10<00:00, 36.68it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  93%|█████████▎| 395/426 [00:10<00:00, 36.79it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  94%|█████████▍| 400/426 [00:10<00:00, 36.90it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  95%|█████████▌| 405/426 [00:10<00:00, 37.01it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  96%|█████████▌| 410/426 [00:11<00:00, 37.12it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  97%|█████████▋| 415/426 [00:11<00:00, 37.23it/s, loss=1.15, v_num=]\n",
            "Epoch 9:  99%|█████████▊| 420/426 [00:11<00:00, 37.29it/s, loss=1.15, v_num=]\n",
            "Epoch 9: 100%|█████████▉| 425/426 [00:11<00:00, 37.31it/s, loss=1.15, v_num=]\n",
            "Epoch 9: 100%|██████████| 426/426 [00:11<00:00, 37.08it/s, loss=1.15, v_num=]\n",
            "Epoch 10:  70%|██████▉   | 298/426 [00:08<00:03, 36.61it/s, loss=1.12, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 10:  70%|███████   | 300/426 [00:08<00:03, 36.32it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  72%|███████▏  | 305/426 [00:08<00:03, 36.46it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  73%|███████▎  | 310/426 [00:08<00:03, 36.61it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  74%|███████▍  | 315/426 [00:08<00:03, 36.75it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  75%|███████▌  | 320/426 [00:08<00:02, 36.90it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  76%|███████▋  | 325/426 [00:08<00:02, 37.04it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  77%|███████▋  | 330/426 [00:08<00:02, 37.08it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  79%|███████▊  | 335/426 [00:09<00:02, 37.06it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  80%|███████▉  | 340/426 [00:09<00:02, 37.06it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  81%|████████  | 345/426 [00:09<00:02, 37.04it/s, loss=1.12, v_num=]\n",
            "Validating:  37%|███▋      | 47/128 [00:01<00:02, 38.21it/s]\u001b[A\n",
            "Epoch 10:  82%|████████▏ | 350/426 [00:09<00:02, 37.02it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  83%|████████▎ | 355/426 [00:09<00:01, 37.13it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  85%|████████▍ | 360/426 [00:09<00:01, 37.22it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  86%|████████▌ | 365/426 [00:09<00:01, 37.34it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  87%|████████▋ | 370/426 [00:09<00:01, 37.45it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  88%|████████▊ | 375/426 [00:09<00:01, 37.56it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  89%|████████▉ | 380/426 [00:10<00:01, 37.68it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  90%|█████████ | 385/426 [00:10<00:01, 37.78it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  92%|█████████▏| 390/426 [00:10<00:00, 37.88it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  93%|█████████▎| 395/426 [00:10<00:00, 37.98it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  94%|█████████▍| 400/426 [00:10<00:00, 38.08it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  95%|█████████▌| 405/426 [00:10<00:00, 38.18it/s, loss=1.12, v_num=]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 10:  96%|█████████▌| 410/426 [00:10<00:00, 38.28it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  97%|█████████▋| 415/426 [00:10<00:00, 38.37it/s, loss=1.12, v_num=]\n",
            "Epoch 10:  99%|█████████▊| 420/426 [00:10<00:00, 38.46it/s, loss=1.12, v_num=]\n",
            "Epoch 10: 100%|██████████| 426/426 [00:11<00:00, 38.35it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  70%|██████▉   | 298/426 [00:08<00:03, 34.72it/s, loss=1.12, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Validating:   0%|          | 0/128 [00:00<?, ?it/s]\u001b[A\n",
            "Epoch 11:  70%|███████   | 300/426 [00:08<00:03, 34.40it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  72%|███████▏  | 305/426 [00:08<00:03, 34.57it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  73%|███████▎  | 310/426 [00:08<00:03, 34.73it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  74%|███████▍  | 315/426 [00:09<00:03, 34.89it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  75%|███████▌  | 320/426 [00:09<00:03, 35.04it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  76%|███████▋  | 325/426 [00:09<00:02, 35.20it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  77%|███████▋  | 330/426 [00:09<00:02, 35.34it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  79%|███████▊  | 335/426 [00:09<00:02, 35.48it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  80%|███████▉  | 340/426 [00:09<00:02, 35.62it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  81%|████████  | 345/426 [00:09<00:02, 35.75it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  82%|████████▏ | 350/426 [00:09<00:02, 35.88it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  83%|████████▎ | 355/426 [00:09<00:01, 35.98it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  85%|████████▍ | 360/426 [00:10<00:01, 35.99it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  86%|████████▌ | 365/426 [00:10<00:01, 35.98it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  87%|████████▋ | 370/426 [00:10<00:01, 35.98it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  88%|████████▊ | 375/426 [00:10<00:01, 35.97it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  89%|████████▉ | 380/426 [00:10<00:01, 36.03it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  90%|█████████ | 385/426 [00:10<00:01, 36.15it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  92%|█████████▏| 390/426 [00:10<00:00, 36.26it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  93%|█████████▎| 395/426 [00:10<00:00, 36.33it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  94%|█████████▍| 400/426 [00:10<00:00, 36.43it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  95%|█████████▌| 405/426 [00:11<00:00, 36.54it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  96%|█████████▌| 410/426 [00:11<00:00, 36.65it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  97%|█████████▋| 415/426 [00:11<00:00, 36.75it/s, loss=1.12, v_num=]\n",
            "Epoch 11:  99%|█████████▊| 420/426 [00:11<00:00, 36.86it/s, loss=1.12, v_num=]\n",
            "Epoch 11: 100%|██████████| 426/426 [00:11<00:00, 36.70it/s, loss=1.12, v_num=]\n",
            "Epoch 12:  70%|██████▉   | 298/426 [00:07<00:03, 37.56it/s, loss=1.11, v_num=]\n",
            "Validating: 0it [00:00, ?it/s]\u001b[A\n",
            "Epoch 12:  70%|███████   | 300/426 [00:08<00:03, 37.23it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  72%|███████▏  | 305/426 [00:08<00:03, 37.37it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  73%|███████▎  | 310/426 [00:08<00:03, 37.51it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  74%|███████▍  | 315/426 [00:08<00:02, 37.64it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  75%|███████▌  | 320/426 [00:08<00:02, 37.76it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  76%|███████▋  | 325/426 [00:08<00:02, 37.85it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  77%|███████▋  | 330/426 [00:08<00:02, 37.94it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  79%|███████▊  | 335/426 [00:08<00:02, 38.06it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  80%|███████▉  | 340/426 [00:08<00:02, 38.17it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  81%|████████  | 345/426 [00:09<00:02, 38.19it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  82%|████████▏ | 350/426 [00:09<00:01, 38.21it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  83%|████████▎ | 355/426 [00:09<00:01, 38.32it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  85%|████████▍ | 360/426 [00:09<00:01, 38.44it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  86%|████████▌ | 365/426 [00:09<00:01, 38.55it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  87%|████████▋ | 370/426 [00:09<00:01, 38.66it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  88%|████████▊ | 375/426 [00:09<00:01, 38.73it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  89%|████████▉ | 380/426 [00:09<00:01, 38.83it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  90%|█████████ | 385/426 [00:09<00:01, 38.93it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  92%|█████████▏| 390/426 [00:09<00:00, 39.03it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  93%|█████████▎| 395/426 [00:10<00:00, 39.10it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  94%|█████████▍| 400/426 [00:10<00:00, 39.20it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  95%|█████████▌| 405/426 [00:10<00:00, 39.29it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  96%|█████████▌| 410/426 [00:10<00:00, 39.38it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  97%|█████████▋| 415/426 [00:10<00:00, 39.46it/s, loss=1.11, v_num=]\n",
            "Epoch 12:  99%|█████████▊| 420/426 [00:10<00:00, 39.55it/s, loss=1.11, v_num=]\n",
            "Epoch 12: 100%|█████████▉| 425/426 [00:10<00:00, 39.64it/s, loss=1.11, v_num=]\n",
            "Epoch 12: 100%|██████████| 426/426 [00:10<00:00, 39.40it/s, loss=1.11, v_num=]\n",
            "Epoch 13:  68%|██████▊   | 291/426 [00:08<00:03, 34.62it/s, loss=1.1, v_num=] "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9Dh6n9j6M8f"
      },
      "source": [
        ""
      ],
      "id": "Q9Dh6n9j6M8f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0cArpFK6M8f"
      },
      "source": [
        ""
      ],
      "id": "_0cArpFK6M8f",
      "execution_count": null,
      "outputs": []
    }
  ]
}